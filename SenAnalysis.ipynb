{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a39dd1e-2411-4fa3-a20e-1a729d9f0d32",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- author: string (nullable = true)\n |-- comment: string (nullable = true)\n |-- published_at: string (nullable = true)\n\nData saved successfully.\nInitial record count: 8489\nRecord count after cleaning: 8489\nRecord count after tokenization: 8489\nRecord count after stop word removal: 8489\nFinal distinct record count: 8479\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from pyspark.sql.functions import col, udf, array_join\n",
    "from pyspark.sql.types import StringType\n",
    "import html\n",
    "import re\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"SentimentAnalysis\").getOrCreate()\n",
    "\n",
    "# Load CSV file into DataFrame\n",
    "file_path = \"/FileStore/tables/cleaned_youtube_comments.csv\"\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Inspect the schema of the DataFrame\n",
    "df.printSchema()\n",
    "\n",
    "# Function to clean HTML tags and decode characters\n",
    "def clean_comment(comment):\n",
    "    if comment is None:\n",
    "        return \"\"\n",
    "    # Decode HTML entities\n",
    "    comment = html.unescape(comment)\n",
    "    # Remove HTML tags\n",
    "    comment = re.sub(r'<.*?>', '', comment)\n",
    "    return comment\n",
    "\n",
    "# Register UDF\n",
    "clean_comment_udf = udf(clean_comment, StringType())\n",
    "\n",
    "# Apply UDF to clean comments\n",
    "df_cleaned = df.withColumn(\"cleaned_comment\", clean_comment_udf(col(\"comment\"))).cache()\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer(inputCol=\"cleaned_comment\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(df_cleaned).cache()\n",
    "\n",
    "# Stop word removal\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "filteredData = remover.transform(wordsData).cache()\n",
    "\n",
    "# Join the elements of the array into a single string\n",
    "finalData = filteredData.withColumn(\"filtered_words_str\", array_join(col(\"filtered_words\"), \" \")).cache()\n",
    "\n",
    "# Remove duplicates in the final DataFrame\n",
    "finalDataDistinct = finalData.dropDuplicates()\n",
    "\n",
    "# Define output path\n",
    "output_path = \"/FileStore/tables/processed_youtube_comments.csv\"\n",
    "\n",
    "# Remove existing directory if exists\n",
    "try:\n",
    "    dbutils.fs.rm(output_path, recurse=True)\n",
    "except Exception as e:\n",
    "    print(f\"Error removing existing directory: {e}\")\n",
    "\n",
    "# Save processed data to CSV\n",
    "try:\n",
    "    finalDataDistinct.select(\"author\", \"comment\", \"filtered_words_str\", \"published_at\").write.mode(\"overwrite\").csv(output_path, header=True)\n",
    "    print(\"Data saved successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error writing data to CSV: {e}\")\n",
    "\n",
    "# Verify record counts at each stage\n",
    "initial_count = df.count()\n",
    "cleaned_count = df_cleaned.count()\n",
    "tokenized_count = wordsData.count()\n",
    "filtered_count = filteredData.count()\n",
    "final_count = finalDataDistinct.count()\n",
    "\n",
    "print(f\"Initial record count: {initial_count}\")\n",
    "print(f\"Record count after cleaning: {cleaned_count}\")\n",
    "print(f\"Record count after tokenization: {tokenized_count}\")\n",
    "print(f\"Record count after stop word removal: {filtered_count}\")\n",
    "print(f\"Final distinct record count: {final_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a17615e-8a45-40e4-a5f0-161e6870a7b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nCollecting nltk\n  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\nCollecting tqdm\n  Downloading tqdm-4.66.4-py3-none-any.whl (78 kB)\nRequirement already satisfied: click in /databricks/python3/lib/python3.9/site-packages (from nltk) (8.0.4)\nCollecting regex>=2021.8.3\n  Downloading regex-2024.5.15-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (774 kB)\nRequirement already satisfied: joblib in /databricks/python3/lib/python3.9/site-packages (from nltk) (1.1.1)\nInstalling collected packages: tqdm, regex, nltk\nSuccessfully installed nltk-3.8.1 regex-2024.5.15 tqdm-4.66.4\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8e71fbd-850d-4a40-a906-c4c0776c815c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- author: string (nullable = true)\n |-- comment: string (nullable = true)\n |-- filtered_words_str: string (nullable = true)\n |-- published_at: string (nullable = true)\n\n+--------------------+--------------------+--------------------+--------------------+\n|              author|             comment|  filtered_words_str|        published_at|\n+--------------------+--------------------+--------------------+--------------------+\n|     @jameswalsh2427|The Best Presente...|best presenter yo...|2024-06-09T00:47:58Z|\n|@circleinforthecu...|i hope we see stu...|hope see stuff ba...|2024-06-01T12:39:58Z|\n|       @kristversoza|\"<a href=\"\"https:...|\"30:01 get rick r...|2024-05-23T11:11:10Z|\n|           @nuk4lear|Me: ferrofluid lo...|me: ferrofluid lo...|2024-04-26T02:19:52Z|\n|@BeautifulRecitat...|Sir can you pleas...|sir please give u...|2024-04-15T15:05:23Z|\n+--------------------+--------------------+--------------------+--------------------+\nonly showing top 5 rows\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+---------+\n|              author|             comment|  filtered_words_str|sentiment|\n+--------------------+--------------------+--------------------+---------+\n|     @jameswalsh2427|The Best Presente...|best presenter yo...| positive|\n|@circleinforthecu...|i hope we see stu...|hope see stuff ba...| positive|\n|       @kristversoza|\"<a href=\"\"https:...|\"30:01 get rick r...|  neutral|\n|           @nuk4lear|Me: ferrofluid lo...|me: ferrofluid lo...| positive|\n|@BeautifulRecitat...|Sir can you pleas...|sir please give u...| positive|\n+--------------------+--------------------+--------------------+---------+\nonly showing top 5 rows\n\n+---------+-----+\n|sentiment|count|\n+---------+-----+\n| positive| 3740|\n|  neutral| 3660|\n| negative| 1079|\n+---------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "processed_file_path = \"/FileStore/tables/processed_youtube_comments.csv\"\n",
    "processed_df = spark.read.csv(processed_file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Display the schema and a few rows to verify the data\n",
    "processed_df.printSchema()\n",
    "processed_df.show(5)\n",
    "\n",
    "# Import necessary libraries for sentiment analysis\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Download VADER lexicon\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Initialize VADER sentiment analyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Define a function to classify sentiment\n",
    "def analyze_sentiment(text):\n",
    "    if text is None:\n",
    "        return \"neutral\"\n",
    "    scores = sid.polarity_scores(text)\n",
    "    if scores['compound'] >= 0.05:\n",
    "        return \"positive\"\n",
    "    elif scores['compound'] <= -0.05:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "# Register the function as a UDF\n",
    "analyze_sentiment_udf = udf(analyze_sentiment, StringType())\n",
    "\n",
    "# Apply the UDF to classify sentiment\n",
    "sentiment_df = processed_df.withColumn(\"sentiment\", analyze_sentiment_udf(col(\"filtered_words_str\")))\n",
    "\n",
    "# Show some examples\n",
    "sentiment_df.select(\"author\", \"comment\", \"filtered_words_str\", \"sentiment\").show(5)\n",
    "\n",
    "# Group by sentiment and count the occurrences\n",
    "sentiment_counts = sentiment_df.groupBy(\"sentiment\").count()\n",
    "\n",
    "# Show the aggregated result\n",
    "sentiment_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6a53f858-ba53-4e87-a12f-467b7ae2bc9e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from pyspark.sql.functions import col, udf, array_join\n",
    "from pyspark.sql.types import StringType\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import html\n",
    "import re\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"SentimentAnalysis\").getOrCreate()\n",
    "\n",
    "# Load CSV file into DataFrame\n",
    "file_path = \"/FileStore/tables/cleaned_youtube_comments.csv\"\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Function to clean HTML tags and decode characters\n",
    "def clean_comment(comment):\n",
    "    if comment is None:\n",
    "        return \"\"\n",
    "    # Decode HTML entities\n",
    "    comment = html.unescape(comment)\n",
    "    # Remove HTML tags\n",
    "    comment = re.sub(r'<.*?>', '', comment)\n",
    "    return comment\n",
    "\n",
    "# Register UDF for cleaning comments\n",
    "clean_comment_udf = udf(clean_comment, StringType())\n",
    "\n",
    "# Apply UDF to clean comments\n",
    "df_cleaned = df.withColumn(\"cleaned_comment\", clean_comment_udf(col(\"comment\")))\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer(inputCol=\"cleaned_comment\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(df_cleaned)\n",
    "\n",
    "# Stop word removal\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "filteredData = remover.transform(wordsData)\n",
    "\n",
    "# Join the elements of the array into a single string\n",
    "finalData = filteredData.withColumn(\"filtered_words_str\", array_join(col(\"filtered_words\"), \" \"))\n",
    "\n",
    "# Sentiment analysis function using VaderSentiment\n",
    "def analyze_sentiment(comment):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    scores = analyzer.polarity_scores(comment)\n",
    "    return scores['compound']\n",
    "\n",
    "# Register UDF for sentiment analysis\n",
    "analyze_sentiment_udf = udf(analyze_sentiment, StringType())\n",
    "\n",
    "# Apply the sentiment analysis UDF to the filtered comments\n",
    "sentiment_df = finalData.withColumn(\"sentiment_score\", analyze_sentiment_udf(col(\"filtered_words_str\")))\n",
    "\n",
    "# Calculate overall sentiment statistics\n",
    "sentiment_stats = sentiment_df.agg({\n",
    "    \"sentiment_score\": \"avg\",\n",
    "    \"sentiment_score\": \"min\",\n",
    "    \"sentiment_score\": \"max\"\n",
    "}).first()\n",
    "\n",
    "# Print the correct aggregated values\n",
    "print(f\"Average Sentiment Score: {sentiment_stats['avg(sentiment_score)']}\")\n",
    "print(f\"Minimum Sentiment Score: {sentiment_stats['min(sentiment_score)']}\")\n",
    "print(f\"Maximum Sentiment Score: {sentiment_stats['max(sentiment_score)']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f030810-d4ec-4c1d-b15c-1274696d18c4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Xebia SenAna-Revised",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
